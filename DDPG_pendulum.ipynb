{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.td3 import td3_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import normal_projection_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import suite_gym\n",
    "import tf_agents\n",
    "\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "##https://github.com/tensorflow/agents/issues/275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"DDPG_pendulum/\"\n",
    "env_name='Pendulum-v0'\n",
    "_DEFAULT_REWARD_SCALE = 1.\n",
    "eval_env_name=None\n",
    "env_load_fn=suite_mujoco.load,\n",
    "num_iterations=70000\n",
    "actor_fc_layers=(128, 128)\n",
    "critic_obs_fc_layers=None\n",
    "critic_action_fc_layers=None\n",
    "critic_joint_fc_layers=(128, 128)\n",
    "num_parallel_environments=1\n",
    "# Params for collect\n",
    "initial_collect_steps=100\n",
    "collect_steps_per_iteration=1\n",
    "replay_buffer_capacity=50000\n",
    "# Params for target update\n",
    "target_update_tau=0.005\n",
    "target_update_period=1\n",
    "# Params for train\n",
    "train_steps_per_iteration=1\n",
    "batch_size=64\n",
    "actor_learning_rate=3e-4\n",
    "critic_learning_rate=3e-4\n",
    "alpha_learning_rate=3e-4\n",
    "td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error\n",
    "gamma=0.99\n",
    "reward_scale_factor=_DEFAULT_REWARD_SCALE\n",
    "gradient_clipping=None\n",
    "use_tf_functions=True\n",
    "# Params for eval\n",
    "num_eval_episodes = 10\n",
    "eval_interval=1000\n",
    "# Params for summaries and logging\n",
    "train_checkpoint_interval=10000\n",
    "policy_checkpoint_interval=5000\n",
    "rb_checkpoint_interval=50000\n",
    "log_interval=1000\n",
    "summary_interval=1000\n",
    "summaries_flush_secs=10\n",
    "debug_summaries=False\n",
    "summarize_grads_and_vars=False\n",
    "eval_metrics_callback=None\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_projection_net(action_spec,\n",
    "                          init_action_stddev=0.35,\n",
    "                          init_means_output_factor=0.1):\n",
    "  del init_action_stddev\n",
    "  return normal_projection_network.NormalProjectionNetwork(\n",
    "      action_spec,\n",
    "      mean_transform=None,\n",
    "      state_dependent_std=True,\n",
    "      init_means_output_factor=init_means_output_factor,\n",
    "      #std_transform=tf.nn.softplus,\n",
    "      scale_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/padmaja/.local/lib/python2.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.expanduser(root_dir)\n",
    "\n",
    "summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      root_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "summary_writer.set_as_default()\n",
    "\n",
    "eval_metrics = [\n",
    "  tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "  tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "py_env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "# create evaluation environment\n",
    "eval_env_name = eval_env_name or env_name\n",
    "eval_py_env = suite_gym.load(eval_env_name)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_step_spec = tf_env.time_step_spec()\n",
    "observation_spec = time_step_spec.observation\n",
    "action_spec = tf_env.action_spec()\n",
    "\n",
    "\n",
    "actor_net = tf_agents.agents.ddpg.actor_network.ActorNetwork(\n",
    "    observation_spec, action_spec, fc_layer_params=actor_fc_layers,\n",
    "    \n",
    ")\n",
    "\n",
    "critic_net = tf_agents.agents.ddpg.critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec), joint_fc_layer_params=critic_joint_fc_layers)\n",
    "\n",
    "\"\"\"actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layers,\n",
    "    continuous_projection_net=normal_projection_net)\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec),\n",
    "    observation_fc_layer_params=critic_obs_fc_layers,\n",
    "    action_fc_layer_params=critic_action_fc_layers,\n",
    "    joint_fc_layer_params=critic_joint_fc_layers)\n",
    "\"\"\"\n",
    "\n",
    "tf_agent = tf_agents.agents.DdpgAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    #alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "     #   learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=td_errors_loss_fn,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=global_step)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No checkpoint available at DDPG_pendulum/train\n",
      "INFO:absl:No checkpoint available at DDPG_pendulum/policy\n",
      "INFO:absl:No checkpoint available at DDPG_pendulum/replay_buffer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make the replay buffer.\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=num_parallel_environments,\n",
    "    max_length=replay_buffer_capacity)\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "env_steps = tf_metrics.EnvironmentSteps(prefix='Train')\n",
    "\n",
    "average_return = tf_metrics.AverageReturnMetric(\n",
    "    prefix='Train',\n",
    "    buffer_size=num_eval_episodes,\n",
    "    batch_size=tf_env.batch_size)\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(prefix='Train'),\n",
    "    env_steps,\n",
    "    average_return,\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        prefix='Train',\n",
    "        buffer_size=num_eval_episodes,\n",
    "        batch_size=tf_env.batch_size),\n",
    "]\n",
    "\n",
    "eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
    "\n",
    "initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
    "    tf_env.time_step_spec(), tf_env.action_spec())\n",
    "\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(root_dir, 'train'),\n",
    "    agent=tf_agent,\n",
    "    global_step=global_step,\n",
    "    metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "policy_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(root_dir, 'policy'),\n",
    "    policy=eval_policy,\n",
    "    global_step=global_step)\n",
    "rb_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=os.path.join(root_dir, 'replay_buffer'),\n",
    "    max_to_keep=1,\n",
    "    replay_buffer=replay_buffer)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "\n",
    "rb_checkpointer.initialize_or_restore()\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=replay_observer + train_metrics,\n",
    "    num_steps=initial_collect_steps)\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    collect_policy,\n",
    "    observers=replay_observer + train_metrics,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "tf_agent.train = common.function(tf_agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initializing replay buffer by collecting experience for 100 stepswith a random policy.\n"
     ]
    }
   ],
   "source": [
    "# Collect initial replay data.\n",
    "if env_steps.result() == 0 or replay_buffer.num_frames() == 0:\n",
    "    logging.info(\n",
    "      'Initializing replay buffer by collecting experience for %d steps'\n",
    "      'with a random policy.', initial_collect_steps)\n",
    "    initial_collect_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "  experience, _ = next(iterator)\n",
    "  return tf_agent.train(experience)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t AverageReturn = -1502.4309082\n",
      "\t\t AverageEpisodeLength = 200.0\n"
     ]
    }
   ],
   "source": [
    "results = metric_utils.eager_compute(\n",
    "    eval_metrics,\n",
    "    eval_tf_env,\n",
    "    eval_policy,\n",
    "    num_episodes=num_eval_episodes,\n",
    "    train_step=env_steps.result(),\n",
    "    summary_writer=summary_writer,\n",
    "    summary_prefix='Eval',\n",
    ")\n",
    "if eval_metrics_callback is not None:\n",
    "  eval_metrics_callback(results, env_steps.result())\n",
    "metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "time_step = None\n",
    "policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "time_acc = 0\n",
    "env_steps_before = env_steps.result().numpy()\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "train_step = common.function(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/padmaja/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/padmaja/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:absl:env steps = 1100, average return = -1533.921143\n",
      "INFO:absl:296.643 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1500.12268066\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 2100, average return = -1540.971924\n",
      "INFO:absl:561.789 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1348.3848877\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 3100, average return = -1436.572510\n",
      "INFO:absl:521.694 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1154.89428711\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 4100, average return = -1227.189453\n",
      "INFO:absl:532.511 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1169.27026367\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 5100, average return = -1003.649536\n",
      "INFO:absl:563.159 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -694.165527344\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-5000\n",
      "INFO:absl:env steps = 6100, average return = -886.632446\n",
      "INFO:absl:545.090 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1001.28186035\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 7100, average return = -915.534851\n",
      "INFO:absl:496.339 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -991.790405273\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 8100, average return = -936.637512\n",
      "INFO:absl:469.220 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -1320.55297852\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 9100, average return = -1037.245850\n",
      "INFO:absl:528.220 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -687.883178711\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 10100, average return = -750.570923\n",
      "INFO:absl:531.915 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -217.52961731\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-10000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-10000\n",
      "INFO:absl:env steps = 11100, average return = -394.584045\n",
      "INFO:absl:484.751 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -217.874069214\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 12100, average return = -443.436523\n",
      "INFO:absl:481.449 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -233.099975586\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 13100, average return = -306.667664\n",
      "INFO:absl:475.087 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -166.117706299\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 14100, average return = -199.095200\n",
      "INFO:absl:480.950 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -148.197372437\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 15100, average return = -223.441650\n",
      "INFO:absl:455.771 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -145.429290771\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-15000\n",
      "INFO:absl:env steps = 16100, average return = -218.758835\n",
      "INFO:absl:459.734 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -145.661392212\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 17100, average return = -229.323456\n",
      "INFO:absl:441.881 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -159.985733032\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 18100, average return = -210.388550\n",
      "INFO:absl:433.418 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -147.770736694\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 19100, average return = -213.235504\n",
      "INFO:absl:416.980 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -111.908103943\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 20100, average return = -175.034851\n",
      "INFO:absl:489.186 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -148.67666626\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-20000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-20000\n",
      "INFO:absl:env steps = 21100, average return = -135.234772\n",
      "INFO:absl:443.671 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -135.153152466\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 22100, average return = -183.053940\n",
      "INFO:absl:456.529 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -157.119857788\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 23100, average return = -183.461624\n",
      "INFO:absl:464.814 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -163.872406006\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 24100, average return = -170.430023\n",
      "INFO:absl:409.765 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -166.933044434\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 25100, average return = -242.762054\n",
      "INFO:absl:423.064 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -168.550567627\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-25000\n",
      "INFO:absl:env steps = 26100, average return = -267.589478\n",
      "INFO:absl:386.355 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -161.924102783\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 27100, average return = -235.372467\n",
      "INFO:absl:424.043 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -147.696563721\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 28100, average return = -234.386230\n",
      "INFO:absl:450.378 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -167.928817749\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 29100, average return = -200.825211\n",
      "INFO:absl:390.421 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -169.364898682\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 30100, average return = -183.634720\n",
      "INFO:absl:408.003 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -144.463607788\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-30000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-30000\n",
      "INFO:absl:env steps = 31100, average return = -188.985489\n",
      "INFO:absl:441.323 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -147.497650146\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 32100, average return = -184.760864\n",
      "INFO:absl:444.167 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -160.161224365\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 33100, average return = -186.654327\n",
      "INFO:absl:486.184 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -134.056640625\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 34100, average return = -211.520798\n",
      "INFO:absl:471.510 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -179.544418335\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 35100, average return = -217.300339\n",
      "INFO:absl:437.595 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -217.531829834\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-35000\n",
      "INFO:absl:env steps = 36100, average return = -177.859146\n",
      "INFO:absl:439.557 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -167.171264648\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 37100, average return = -171.837570\n",
      "INFO:absl:440.406 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -157.29864502\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 38100, average return = -174.467331\n",
      "INFO:absl:413.658 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -131.060638428\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 39100, average return = -160.411713\n",
      "INFO:absl:406.978 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -126.082809448\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 40100, average return = -159.028275\n",
      "INFO:absl:438.405 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -112.03855896\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-40000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-40000\n",
      "INFO:absl:env steps = 41100, average return = -122.717331\n",
      "INFO:absl:485.406 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -132.581329346\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 42100, average return = -123.359787\n",
      "INFO:absl:420.008 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -131.559539795\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 43100, average return = -220.461716\n",
      "INFO:absl:438.755 env steps/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t AverageReturn = -109.646095276\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 44100, average return = -207.247879\n",
      "INFO:absl:417.885 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -168.162322998\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 45100, average return = -170.019440\n",
      "INFO:absl:494.639 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -96.2101287842\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-45000\n",
      "INFO:absl:env steps = 46100, average return = -159.436523\n",
      "INFO:absl:425.673 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -117.88470459\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 47100, average return = -124.757568\n",
      "INFO:absl:496.165 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -121.214233398\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 48100, average return = -170.558868\n",
      "INFO:absl:484.450 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -94.6641082764\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 49100, average return = -204.330505\n",
      "INFO:absl:498.970 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -124.596191406\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 50100, average return = -144.985550\n",
      "INFO:absl:554.735 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -138.796020508\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-50000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-50000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/replay_buffer/ckpt-50000\n",
      "INFO:absl:env steps = 51100, average return = -180.386307\n",
      "INFO:absl:502.197 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -188.067962646\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 52100, average return = -252.973999\n",
      "INFO:absl:504.216 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -120.372520447\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 53100, average return = -210.714157\n",
      "INFO:absl:514.618 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -135.65625\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 54100, average return = -199.136017\n",
      "INFO:absl:518.187 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -103.804855347\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 55100, average return = -222.029831\n",
      "INFO:absl:503.495 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -137.560150146\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-55000\n",
      "INFO:absl:env steps = 56100, average return = -272.326569\n",
      "INFO:absl:529.721 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -140.589187622\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 57100, average return = -313.369781\n",
      "INFO:absl:544.991 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -119.843261719\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 58100, average return = -328.266205\n",
      "INFO:absl:482.524 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -163.011642456\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 59100, average return = -237.893158\n",
      "INFO:absl:488.959 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -177.935653687\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 60100, average return = -141.203629\n",
      "INFO:absl:479.422 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -150.330245972\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-60000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-60000\n",
      "INFO:absl:env steps = 61100, average return = -165.648956\n",
      "INFO:absl:481.440 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -170.883651733\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 62100, average return = -162.002411\n",
      "INFO:absl:501.842 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -114.57069397\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 63100, average return = -190.141388\n",
      "INFO:absl:474.475 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -103.665870667\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 64100, average return = -197.841217\n",
      "INFO:absl:491.462 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -198.715362549\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 65100, average return = -157.887619\n",
      "INFO:absl:479.543 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -177.537078857\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-65000\n",
      "INFO:absl:env steps = 66100, average return = -186.857864\n",
      "INFO:absl:475.588 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -157.455337524\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 67100, average return = -211.338791\n",
      "INFO:absl:425.711 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -132.723236084\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 68100, average return = -163.034637\n",
      "INFO:absl:463.921 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -151.863327026\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 69100, average return = -195.097305\n",
      "INFO:absl:474.454 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -287.463470459\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:env steps = 70100, average return = -271.911377\n",
      "INFO:absl:482.176 env steps/sec\n",
      "INFO:absl: \n",
      "\t\t AverageReturn = -191.587051392\n",
      "\t\t AverageEpisodeLength = 200.0\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/train/ckpt-70000\n",
      "INFO:absl:Saved checkpoint: DDPG_pendulum/policy/ckpt-70000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "  start_time = time.time()\n",
    "  time_step, policy_state = collect_driver.run(\n",
    "      time_step=time_step,\n",
    "      policy_state=policy_state,\n",
    "  )\n",
    "  for _ in range(train_steps_per_iteration):\n",
    "    train_step()\n",
    "  time_acc += time.time() - start_time\n",
    "\n",
    "  if global_step.numpy() % log_interval == 0:\n",
    "    logging.info('env steps = %d, average return = %f', env_steps.result(),\n",
    "                 average_return.result())\n",
    "    env_steps_per_sec = (env_steps.result().numpy() -\n",
    "                         env_steps_before) / time_acc\n",
    "    logging.info('%.3f env steps/sec', env_steps_per_sec)\n",
    "    tf.compat.v2.summary.scalar(\n",
    "        name='env_steps_per_sec',\n",
    "        data=env_steps_per_sec,\n",
    "        step=env_steps.result())\n",
    "    time_acc = 0\n",
    "    env_steps_before = env_steps.result().numpy()\n",
    "\n",
    "  for train_metric in train_metrics:\n",
    "    train_metric.tf_summaries(train_step=env_steps.result())\n",
    "\n",
    "  if global_step.numpy() % eval_interval == 0:\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=env_steps.result(),\n",
    "        summary_writer=summary_writer,\n",
    "        summary_prefix='Eval',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, env_steps.result())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "    global_step_val = global_step.numpy()\n",
    "    if global_step_val % train_checkpoint_interval == 0:\n",
    "        train_checkpointer.save(global_step=global_step_val)\n",
    "    if global_step_val % policy_checkpoint_interval == 0:\n",
    "        policy_checkpointer.save(global_step=global_step_val)\n",
    "    if global_step_val % rb_checkpoint_interval == 0:\n",
    "        rb_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
